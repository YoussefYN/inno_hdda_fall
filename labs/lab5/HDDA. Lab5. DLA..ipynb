{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDDA. Lab5. LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrimination & Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_blobs(n_samples=400, n_features=2, cluster_std=1.0,\n",
    "                  centers=[(-5, -5), (0, 0), (5, 5)], shuffle=False, random_state=42)\n",
    "color_map = dict(zip(np.unique(Y), [\"red\", \"blue\", \"green\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{plt.scatter(X[Y == k, 0], X[Y == k, 1], c=v, s=10, edgecolor='k') for (k,v) in color_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = list(zip(X, Y))\n",
    "bycategory = list()\n",
    "for j in np.unique(Y):\n",
    "    bycategory.append([k[1] for (k,v) in A if v == j])\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.boxplot(bycategory)\n",
    "plt.title('ANOVA')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.boxplot(bycategory, 0, 'rs', 0)\n",
    "plt.title('LDA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is like inverted ANOVA: ANOVA looks for differences in a continuous response among categories, whereas LDA infers categories using a continuous predictor. LDA assumes that the variance in each group is the same, and that the predictor(s) are normally distributed for each group. In other words, different $\\mu_k$, one shared $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdfs = list()\n",
    "for i in np.unique(Y):\n",
    "    cdfs.append(np.linspace(min(bycategory[i]), max(bycategory[i]), 50))\n",
    "plt.figure(figsize=(10, 4))\n",
    "k = 1\n",
    "for x in cdfs:\n",
    "    mu_k = x.mean()\n",
    "    plt.plot(x, stats.norm.pdf(x, loc=mu_k))\n",
    "    plt.plot([mu_k, mu_k], [0, 0.5], c='k')\n",
    "    plt.text(mu_k + 0.2, 0.5, \"$\\mu_%i$\" % k, size=18)\n",
    "    k += 1\n",
    "plt.ylim(0, 0.75)\n",
    "\n",
    "plt.xlabel('Predictor', size=18)\n",
    "plt.ylabel('Probability', size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(solver='lsqr', shrinkage=None).fit(X, Y)\n",
    "k_fold = cross_validation.KFold(len(A), 3, shuffle=True)\n",
    "print('LDA Results: ')\n",
    "for (trn, tst) in k_fold:\n",
    "    lda.fit(X[trn], Y[trn])\n",
    "    outVal = lda.score(X[tst], Y[tst])\n",
    "\n",
    "print('Score: ' + str(outVal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA as projection method\n",
    "[Comparison of LDA and PCA 2D projection of Iris dataset](http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "X_r2 = lda.fit(X, y).transform(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure()\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of IRIS dataset')\n",
    "\n",
    "plt.figure()\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA of IRIS dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides a visual summary of the confusion matrix over a range of criteria. Given a confusion matrix, $N=TN+FP$, $P=TP+FN$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.text(0.25, 0.75, 'TN', size=18)\n",
    "plt.text(0.75, 0.75, 'FP', size=18)\n",
    "plt.text(0.25, 0.25, 'FN', size=18)\n",
    "plt.text(0.75, 0.25, 'TN', size=18)\n",
    "\n",
    "plt.xticks([0.25, 0.75], ['Neg', 'Pos'], size=20)\n",
    "plt.yticks([0.25, 0.75], ['Pos', 'Neg'], size=20)\n",
    "plt.ylabel('Truth', size=24)\n",
    "plt.xlabel('Prediction', size=24)\n",
    "plt.title('Confusion Matrix', size=26)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true positive rate, or Power (or Sensitivity) is $\\frac{TP}{P}$ and the Type 1 error is $\\frac{FP}{N}$. The ROC curve shows Power vs. Type 1 error. Ideally, we can achieve a high true positive rate at a very low false positive rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ROC curve](http://arogozhnikov.github.io/2015/10/05/roc-curve.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast cancer ROC demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=44)\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', C=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Session Task:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider `Wine.csv`. First 13 columns i.e. independent variables are contents of wine. Last column is customer segment. It has 3 values. This is a classification problem. Our objective is to reduce 13 independent variables to `n_components`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Wine.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: determine an effective dimension reduction (EDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: use you EDR for PCA n_components in a template below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 0:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE YOUR  EDR HERE (n_components) \n",
    "\n",
    "pca = PCA(n_components = ???)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sn.heatmap(cm, annot=True,annot_kws={\"size\": 16})# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set, y_set = X_test, y_test\n",
    " \n",
    "aranged_pc1 = np.arange(start = X_set[:, 0].min(), stop = X_set[:, 0].max(), step = 0.01)\n",
    "aranged_pc2 = np.arange(start = X_set[:, 1].min(), stop = X_set[:, 1].max(), step = 0.01)\n",
    " \n",
    "X1, X2 = np.meshgrid(aranged_pc1, aranged_pc2)\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "alpha = 0.5, cmap = ListedColormap(('orange', 'blue', 'green')))\n",
    " \n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green','blue'))(i), label = j)\n",
    "plt.title('Principal Component Analysis')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: use you EDR for LDA n_components in a template below. Compare LDA with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 0:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE YOUR  EDR HERE (n_components) \n",
    "\n",
    "pca = PCA(n_components = ???)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sn.heatmap(cm, annot=True,annot_kws={\"size\": 16})# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set, y_set = X_test, y_test\n",
    " \n",
    "aranged_pc1 = np.arange(start = X_set[:, 0].min(), stop = X_set[:, 0].max(), step = 0.01)\n",
    "aranged_pc2 = np.arange(start = X_set[:, 1].min(), stop = X_set[:, 1].max(), step = 0.01)\n",
    " \n",
    "X1, X2 = np.meshgrid(aranged_pc1, aranged_pc2)\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "alpha = 0.5, cmap = ListedColormap(('orange', 'blue', 'green')))\n",
    " \n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green','blue'))(i), label = j)\n",
    "plt.title('Linear Discriminant Analysis')\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA performing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage\n",
    "[Normal and Shrinkage Linear Discriminant Analysis for classification](http://scikit-learn.org/stable/auto_examples/classification/plot_lda.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "n_train = 20  # samples for training\n",
    "n_test = 200  # samples for testing\n",
    "n_averages = 50  # how often to repeat classification\n",
    "n_features_max = 75  # maximum number of features\n",
    "step = 4  # step size for the calculation\n",
    "\n",
    "\n",
    "def generate_data(n_samples, n_features):\n",
    "    \"\"\"Generate random blob-ish data with noisy features.\n",
    "\n",
    "    This returns an array of input data with shape `(n_samples, n_features)`\n",
    "    and an array of `n_samples` target labels.\n",
    "\n",
    "    Only one feature contains discriminative information, the other features\n",
    "    contain only noise.\n",
    "    \"\"\"\n",
    "    X, y = make_blobs(n_samples=n_samples, n_features=1, centers=[[-2], [2]])\n",
    "\n",
    "    # add non-discriminative features\n",
    "    if n_features > 1:\n",
    "        X = np.hstack([X, np.random.randn(n_samples, n_features - 1)])\n",
    "    return X, y\n",
    "\n",
    "acc_clf1, acc_clf2 = [], []\n",
    "n_features_range = range(1, n_features_max + 1, step)\n",
    "for n_features in n_features_range:\n",
    "    score_clf1, score_clf2 = 0, 0\n",
    "    for _ in range(n_averages):\n",
    "        X, y = generate_data(n_train, n_features)\n",
    "\n",
    "        clf1 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto').fit(X, y)\n",
    "        clf2 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=None).fit(X, y)\n",
    "\n",
    "        X, y = generate_data(n_test, n_features)\n",
    "        score_clf1 += clf1.score(X, y)\n",
    "        score_clf2 += clf2.score(X, y)\n",
    "\n",
    "    acc_clf1.append(score_clf1 / n_averages)\n",
    "    acc_clf2.append(score_clf2 / n_averages)\n",
    "\n",
    "features_samples_ratio = np.array(n_features_range) / n_train\n",
    "\n",
    "plt.plot(features_samples_ratio, acc_clf1, linewidth=2,\n",
    "         label=\"Linear Discriminant Analysis with shrinkage\", color='navy')\n",
    "plt.plot(features_samples_ratio, acc_clf2, linewidth=2,\n",
    "         label=\"Linear Discriminant Analysis\", color='gold')\n",
    "\n",
    "plt.xlabel('n_features / n_samples')\n",
    "plt.ylabel('Classification accuracy')\n",
    "\n",
    "plt.legend(loc=1, prop={'size': 12})\n",
    "plt.suptitle('Linear Discriminant Analysis vs. \\\n",
    "shrinkage Linear Discriminant Analysis (1 discriminative feature)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LP problem\n",
    "[Linear and Quadratic Discriminant Analysis](http://scikit-learn.org/stable/modules/lda_qda.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Quadratic Discriminant Analysis** (QDA): Each class uses its own estimate of variance (or covariance when there are multiple input variables).\n",
    "- **Flexible Discriminant Analysis** (FDA): Where non-linear combinations of inputs is used such as splines.\n",
    "- **Regularized Discriminant Analysis** (RDA): Introduces regularization into the estimate of the variance (actually covariance), moderating the influence of different variables on LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# #############################################################################\n",
    "# Colormap\n",
    "cmap = colors.LinearSegmentedColormap(\n",
    "    'red_blue_classes',\n",
    "    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
    "     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
    "     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
    "plt.cm.register_cmap(cmap=cmap)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate datasets\n",
    "def dataset_fixed_cov():\n",
    "    '''Generate 2 Gaussians samples with the same covariance matrix'''\n",
    "    n, dim = 300, 2\n",
    "    np.random.seed(0)\n",
    "    C = np.array([[0., -0.23], [0.83, .23]])\n",
    "    X = np.r_[np.dot(np.random.randn(n, dim), C),\n",
    "              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def dataset_cov():\n",
    "    '''Generate 2 Gaussians samples with different covariance matrices'''\n",
    "    n, dim = 300, 2\n",
    "    np.random.seed(0)\n",
    "    C = np.array([[0., -1.], [2.5, .7]]) * 2.\n",
    "    X = np.r_[np.dot(np.random.randn(n, dim), C),\n",
    "              np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Plot functions\n",
    "def plot_data(lda, X, y, y_pred, fig_index):\n",
    "    splot = plt.subplot(2, 2, fig_index)\n",
    "    if fig_index == 1:\n",
    "        plt.title('Linear Discriminant Analysis')\n",
    "        plt.ylabel('Data with\\n fixed covariance')\n",
    "    elif fig_index == 2:\n",
    "        plt.title('Quadratic Discriminant Analysis')\n",
    "    elif fig_index == 3:\n",
    "        plt.ylabel('Data with\\n varying covariances')\n",
    "\n",
    "    tp = (y == y_pred)  # True Positive\n",
    "    tp0, tp1 = tp[y == 0], tp[y == 1]\n",
    "    X0, X1 = X[y == 0], X[y == 1]\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "\n",
    "    alpha = 0.5\n",
    "\n",
    "    # class 0: dots\n",
    "    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', alpha=alpha,\n",
    "             color='red', markeredgecolor='k')\n",
    "    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '*', alpha=alpha,\n",
    "             color='#990000', markeredgecolor='k')  # dark red\n",
    "\n",
    "    # class 1: dots\n",
    "    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', alpha=alpha,\n",
    "             color='blue', markeredgecolor='k')\n",
    "    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '*', alpha=alpha,\n",
    "             color='#000099', markeredgecolor='k')  # dark blue\n",
    "\n",
    "    # class 0 and 1 : areas\n",
    "    nx, ny = 200, 100\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                         np.linspace(y_min, y_max, ny))\n",
    "    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z[:, 1].reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n",
    "                   norm=colors.Normalize(0., 1.))\n",
    "    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='k')\n",
    "\n",
    "    # means\n",
    "    plt.plot(lda.means_[0][0], lda.means_[0][1],\n",
    "             'o', color='black', markersize=10, markeredgecolor='k')\n",
    "    plt.plot(lda.means_[1][0], lda.means_[1][1],\n",
    "             'o', color='black', markersize=10, markeredgecolor='k')\n",
    "\n",
    "    return splot\n",
    "\n",
    "\n",
    "def plot_ellipse(splot, mean, cov, color):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    u = w[0] / linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at 2 standard deviation\n",
    "    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n",
    "                              180 + angle, facecolor=color,\n",
    "                              edgecolor='yellow',\n",
    "                              linewidth=2, zorder=2)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(0.5)\n",
    "    splot.add_artist(ell)\n",
    "    splot.set_xticks(())\n",
    "    splot.set_yticks(())\n",
    "\n",
    "\n",
    "def plot_lda_cov(lda, splot):\n",
    "    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n",
    "    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n",
    "\n",
    "\n",
    "def plot_qda_cov(qda, splot):\n",
    "    plot_ellipse(splot, qda.means_[0], qda.covariances_[0], 'red')\n",
    "    plot_ellipse(splot, qda.means_[1], qda.covariances_[1], 'blue')\n",
    "\n",
    "for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):\n",
    "    # Linear Discriminant Analysis\n",
    "    lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "    y_pred = lda.fit(X, y).predict(X)\n",
    "    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)\n",
    "    plot_lda_cov(lda, splot)\n",
    "    plt.axis('tight')\n",
    "\n",
    "    # Quadratic Discriminant Analysis\n",
    "    qda = QuadraticDiscriminantAnalysis(store_covariances=True)\n",
    "    y_pred = qda.fit(X, y).predict(X)\n",
    "    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)\n",
    "    plot_qda_cov(qda, splot)\n",
    "    plt.axis('tight')\n",
    "plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant'\n",
    "             'Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Assignment. Linear DA vs. Quadratic DA. (for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('./Hemocrit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Implement your LDA function using template below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    def __init__(self, n_components=None):\n",
    "        # YOUR CODE HERE\n",
    "    def fit(self, X, y):\n",
    "        # YOUR CODE HERE\n",
    "    def predict(self, X):\n",
    "        # YOUR CODE HERE\n",
    "    def fit_transform(self, X, y=None):\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Implement your QDA function using template below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA():\n",
    "    def __init__(self, n_components=None):\n",
    "        # YOUR CODE HERE\n",
    "    def fit(self, X, y):\n",
    "        # YOUR CODE HERE\n",
    "    def predict(self, X):\n",
    "        # YOUR CODE HERE\n",
    "    def fit_transform(self, X, y=None):\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Dataset prediction\n",
    "- Separate your `dataset` into train, test subsets\n",
    "- Predict by using LDA\n",
    "- Predict by using QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Plot ROC curves for LDA and QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('LDA ROC curve')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "plt.ylabel('TPR (power)')\n",
    "plt.xlabel('FPR (type 1 error)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('QDA ROC curve')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "plt.ylabel('TPR (power)')\n",
    "plt.xlabel('FPR (type 1 error)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Assignment. Addition for champions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra task for champions\n",
    "- Perform your LDA by second reguralization parameter (Shrinkage)\n",
    "- Perform your QDA by second reguralization parameter (Shrinkage)\n",
    "- Demonstrate how does it work on `make_blobs` example (see template)\n",
    "\n",
    "[template](http://scikit-learn.org/stable/auto_examples/classification/plot_lda.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
